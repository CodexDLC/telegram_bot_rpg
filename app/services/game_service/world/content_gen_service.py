import asyncio
import json
import traceback  # ðŸ”¥ Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾ Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸

from loguru import logger as log

from app.services.gemini_service.gemini_service import gemini_answer
from database.repositories import IWorldRepo


class ContentGenerationService:
    def __init__(self, world_repo: IWorldRepo):
        self.repo = world_repo
        self.batch_size = 5

    async def generate_content_for_path(self, path_coords: list[tuple[int, int]]):
        """
        Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾, Ð¿Ð°Ñ‡ÐºÐ°Ð¼Ð¸ Ð¿Ð¾ 5 ÑˆÑ‚ÑƒÐº.
        """
        total_nodes = len(path_coords)
        log.info(f"ContentGen | task=started nodes={total_nodes} batch_size={self.batch_size} mode=sequential")

        # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° Ð¼ÐµÐ»ÐºÐ¸Ðµ Ð±Ð°Ñ‚Ñ‡Ð¸
        chunks = [path_coords[i : i + self.batch_size] for i in range(0, total_nodes, self.batch_size)]

        log.info(f"ContentGen | action=batching total_batches={len(chunks)}")

        # ðŸ”¥ FIX: Ð¡Ñ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°
        for i, chunk in enumerate(chunks):
            log.info(f"ContentGen | processing_batch {i + 1}/{len(chunks)}...")
            await self._process_batch(chunk, batch_id=i + 1)
            # ÐŸÐ°ÑƒÐ·Ð°, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ ÑÐ»Ð¾Ð²Ð¸Ñ‚ÑŒ 429
            await asyncio.sleep(1.0)

        log.info("ContentGen | task=finished")

    async def _process_batch(self, chunk: list[tuple[int, int]], batch_id: int):
        payload = []

        # 1. Ð¡Ð‘ÐžÐ  ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢Ð
        for x, y in chunk:
            node = await self.repo.get_node(x, y)
            if not node:
                continue

            # Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ðµ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐ³Ð¾Ð²
            my_tags = []
            if node.flags and isinstance(node.flags, dict):
                my_tags = node.flags.get("biome_tags", [])

            surroundings = await self._get_surroundings_context(x, y)

            item = {
                "id": f"{x}_{y}",
                "internal_tags": my_tags,
                "surroundings": surroundings,
                "fill_content": {
                    "title": "",
                    "description": "",
                },
            }
            payload.append(item)

        if not payload:
            return

        # 2. Ð—ÐÐŸÐ ÐžÐ¡ Ðš GEMINI
        response_text = ""
        try:
            log.debug(f"ContentGen | batch={batch_id} action=sending_request items={len(payload)}")

            response_text = await gemini_answer(
                mode="batch_location_desc",
                user_text=json.dumps(payload, ensure_ascii=False),
                max_tokens=4000,  # ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ 5 ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²
            )

            # Ð§Ð¸ÑÑ‚ÐºÐ° JSON
            clean_json = response_text.replace("```json", "").replace("```", "").strip()
            if not clean_json:
                raise ValueError("Empty response from LLM")

            result_map = json.loads(clean_json)

        except json.JSONDecodeError:
            log.error(
                f"ContentGen | batch={batch_id} status=llm_error err='JSON Decode Failed' raw_preview='{response_text[:100]}'"
            )
            return
        except (ValueError, TypeError) as e:
            # ðŸ”¥ FIX: ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ‚Ñ€ÐµÐ¹ÑÐ±ÐµÐº Ð¾ÑˆÐ¸Ð±ÐºÐ¸, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð½ÑÑ‚ÑŒ, Ñ‡Ñ‚Ð¾ Ð·Ð° 'error'
            log.error(f"ContentGen | batch={batch_id} status=llm_error exception='{e}'")
            log.error(traceback.format_exc())
            return

        # 3. Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ˜Ð•
        saved_count = 0
        for loc_id, content in result_map.items():
            try:
                x, y = map(int, loc_id.split("_"))

                original_item = next((i for i in payload if i["id"] == loc_id), None)
                tags = original_item["internal_tags"] if original_item else []

                final_content = {
                    "title": content.get("title", "ÐŸÑƒÑÑ‚Ð¾ÑˆÑŒ"),
                    "description": content.get("description", "..."),
                    "environment_tags": tags,
                }

                node_db = await self.repo.get_node(x, y)
                sec_id = node_db.sector_id if node_db else "D4"

                await self.repo.create_or_update_node(
                    x=x,
                    y=y,
                    sector_id=sec_id,
                    content=final_content,
                    is_active=True,
                )
                saved_count += 1

            except (ValueError, TypeError, KeyError) as e:
                log.error(f"ContentGen | save_error id={loc_id} err={e}")

        log.info(f"ContentGen | batch={batch_id} status=saved count={saved_count}/{len(chunk)}")

    async def _get_surroundings_context(self, x: int, y: int) -> dict[str, list[str]]:
        directions = {"north": (0, -1), "south": (0, 1), "west": (-1, 0), "east": (1, 0)}
        result = {}

        for dir_name, (dx, dy) in directions.items():
            nx, ny = x + dx, y + dy
            node = await self.repo.get_node(nx, ny)

            if node:
                tags = []
                if node.flags and isinstance(node.flags, dict):
                    tags = node.flags.get("biome_tags", [])
                    if node.flags.get("is_safe_zone"):
                        tags.append("safe_zone")

                if tags:
                    result[dir_name] = tags
            else:
                result[dir_name] = ["void"]

        return result
